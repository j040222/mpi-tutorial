 <!DOCTYPE html>
<html lang="en">
   <meta charset="UTF-8">
   <title>C++ MPI Tutorial</title>
   <meta name="viewport" content="width=device-width,initial-scale=1">
   
   <link rel="stylesheet" href="./styles.css">
   <link
      rel="stylesheet"
      type="text/css"
      href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.5.0/semantic.min.css"
      >
   <link
      rel="stylesheet"
      type="text/css"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/default.min.css"
      >
   
   <script
      src="https://code.jquery.com/jquery-3.1.1.min.js"
      integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8="
      crossorigin="anonymous">
   </script>
   <script
      src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.5.0/semantic.min.js"
      >
   </script>
   <script
      src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"
      >
   </script>
   
   <body>
      <div class="content-column">
         <h1>C++ MPI Tutorial</h1>
         
         <br/>
         
         <p>
         Tutorials and examples of C++ MPI features.
         </p>
         
         <br/>
         
         <p><i class="angle right icon"></i>Contents</p>
         
         <div class="toc">
            <div class="toc-item">
               <a href="#initializing_the_mpi_environment">
                  Initializing the MPI Environment
               </a>
            </div>
            <div class="toc-item">
               <a href="#sending_mpi_messages">
                  Sending Messages from One Process to
                  Another
               </a>
            </div>
            <div class="toc-item">
               <a href="#attaching_a_debugger">
                  Attaching a Debugger
               </a>
            </div>
            <div class="toc-item">
               <a href="#probing_message_sizes">
                  Probing Message Sizes
               </a>
            </div>
            <div class="toc-item">
               <a href="#broadcasting_data">
                  Broadcasting Data
               </a>
            </div>
            <div class="toc-item">
               <a href="#scattering_data">
                  Scattering Data
               </a>
            </div>
            <div class="toc-item toc-level-1">
               <a href="#scattering_data_of_variable_lengths">
                  Using MPI_Scatterv
               </a>
            </div>
            <div class="toc-item">
               <a href="#gathering_data">
                  Gathering Data
               </a>
            </div>
            <div class="toc-item toc-level-1">
               <a href="#using_mpi_gatherv">
                  Using MPI_Gatherv
               </a>
            </div>
            <div class="toc-item toc-level-1">
               <a href="#using_mpi_allgather">
                  Using MPI_Allgather
               </a>
            </div>
            <div class="toc-item">
               <a href="#parallel_reduction">
                  Parallel Reduction
               </a>
            </div>
            <div class="toc-item toc-level-1">
               <a href="#using_mpi_reduce_scatter">
                  Using MPI_Reduce_scatter
               </a>
            </div>
            <div class="toc-item">
               <a href="#all_to_all">
                  All to All
               </a>
            </div>
            <div class="toc-item">
               <a href="#nonblocking_send_receive">
                  Nonblocking Send/Receive, and Barriers
               </a>
            </div>
         </div>
         
         <h2 id="initializing_the_mpi_environment">
            Initializing the MPI Environment
         </h2>
         
         <p>
         MPI programs are usually deployed using a
         program named <program_name>mpiexec</program_name>,
         which is part of the MPI standard.
         </p>
         
         <p>
         <program_name>mpiexec</program_name> accepts
         command line arguments (<i>-n/-np</i>) that
         specify how many processes to launch. However, in
         this example, we will specify the total number of
         processes using a <i>hosts</i> file.
         </p>
         
         <p>
         A <i>hosts</i> file is a simple text file
         containing one record per line with the following
         format and meaning:
         </p>
         
         <div class='text_file'>hostname:number_of_processes</div>
         
         <p>
         <i>hostname</i> (eg. <i>localhost</i>) is the host
         name of the device on which to start
         <i>number_of_processes</i> processes.
         </p>
         
         <p>
         Create a text file named <i>hosts</i> containing
         the following:
         </p>
         
         <div class='text_file'>localhost:5</div>
         
         <p>
         This will request 5 MPI processes all running on
         the local computer.
         </p>
         
         <p>
         Now create a C++ codefile named
         <i>application.cpp</i> with the following content:
         </p>
         
         <pre><code>#include &lt;mpi.h&gt;
#include &lt;iostream&gt;

int main(int argc, char** argv)
{
   //
   // Initialize the MPI environment:
   //
   
   MPI_Init(NULL, NULL);
   
   //
   // Get the total number of processes in the group:
   //
   
   int
      world_size;
      
   MPI_Comm_size(MPI_COMM_WORLD, &world_size);
   
   //
   // Get the rank of this process:
   //
   
   int
      proc_rank;
   
   MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);
   
   //
   // Get the name of the processor running this process:
   //
   
   char
      processor_name[MPI_MAX_PROCESSOR_NAME];
   int
      name_number_of_chars;
   
   MPI_Get_processor_name
      (
      processor_name
         ,
      &name_number_of_chars
      )
      ;
   
   //
   // Print a message:
   //
   
   ::std::cout &lt;&lt; "Hello from processor '"
             &lt;&lt; processor_name
             &lt;&lt; "', rank "
             &lt;&lt; proc_rank
             &lt;&lt; "/"
             &lt;&lt; world_size
             &lt;&lt; " processors"
             &lt;&lt; ::std::endl
                ;
   
   //
   // Finalize the MPI environment before shutting it down:
   //
   
   MPI_Finalize();
   
   return 0;
}
</code></pre>
         
         <p>
         Compile the application using
         <program_name>mpic++</program_name> as follows:
         </p>
         
         <div class='command_line'>mpic++ ./application.cpp -o ./application</div>
         
         <p>
         Run the executable as follows:
         </p>
         
         <div class='command_line'>mpiexec -f ./hosts ./application</div>
         
         <p>
         <program_name>-f hosts</program_name> tells the MPI
         environment to read the process list from
         <i>./hosts</i> (which spawns 5 processes on the
         localhost).
         </p>
         
         <p>
         Possible program output:
         </p>
         
         <div class='program_output'>Hello from processor 'sysadm-wu', rank 0/5 processors
Hello from processor 'sysadm-wu', rank 3/5 processors
Hello from processor 'sysadm-wu', rank 1/5 processors
Hello from processor 'sysadm-wu', rank 2/5 processors
Hello from processor 'sysadm-wu', rank 4/5 processors</div>
         
         <p>
         The order you see may differ.
         </p>
         
         
         <h2 id="sending_mpi_messages">
            Sending Messages from One Process to Another
         </h2>
         
         <p>
         
         Use <function_name>MPI_Send</function_name> and
         <function_name>MPI_Receive</function_name> to send
         data directly from one process to another.
         
         </p>
         
         <p>
         
         The
         sender (the process that calls
         <function_name>MPI_Send</function_name>) can
         specify the rank of the receiving process.
         
         </p>
         
         <p>
         
         In this first example we will send one integer
         (with value <funtion_name>42</funtion_name>) from
         the process with rank 0 to the process with rank 1:
         
         </p>
         
         <pre><code>#include &lt;mpi.h&gt;
#include &lt;iostream&gt;

int main(int argc, char** argv)
{
   MPI_Init(NULL, NULL);
   
   int
      world_size;
   
   MPI_Comm_size(MPI_COMM_WORLD, &world_size);
   
   int
      proc_rank;
   
   MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);
   
   if (
         proc_rank == 0
      )
   {
      //
      // Sender:
      //
      
      int const
         number_to_send = 42;
      
      MPI_Send
         (
         &number_to_send,
         1,
         MPI_INT,
         1, 0,
         MPI_COMM_WORLD
         )
         ;
      
      ::std::cout &lt;&lt; "Process 0 sent "
                  &lt;&lt; number_to_send
                  &lt;&lt; " to process 1"
                  &lt;&lt; ::std::endl
                     ;
   }
   else if
      (
         proc_rank == 1
      )
   {
      //
      // Receiver:
      //
      
      int
         number_received = 0;
      
      MPI_Recv
         (
         &number_received,
         1,
         MPI_INT,
         0, 0,
         MPI_COMM_WORLD,
         MPI_STATUS_IGNORE
         )
         ;
      
   ::std::cout &lt;&lt; "Process 1 received "
               &lt;&lt; number_received
               &lt;&lt; " from process 0"
               &lt;&lt; ::std::endl
                  ;
   }
   
   MPI_Finalize();
   
   return 0;
}
</code></pre>
         
         <p>
         
         Note that the receiving process knew how many
         integers (1) would be received. The second
         argument to <function_name>MPI_Recv</function_name>
         is the <i>maximum</i> number of records that will
         be received, not the number that were sent.
         
         </p>
         <p>
         
         If the number of records sent is not fixed, use
         <function_name>MPI_Status</function_name> and 
         <function_name>MPI_Get_count</function_name>, as
         follows, to query the length of the message:
         
         </p>
         
         <pre><code>#include &lt;mpi.h&gt;
#include &lt;iostream&gt;

int main(int argc, char** argv)
{
   MPI_Init(NULL, NULL);
   
   int
      world_size;
   
   MPI_Comm_size(MPI_COMM_WORLD, &world_size);
   
   int
      proc_rank;
   
   MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);
   
   if (
         proc_rank == 0
      )
   {
      //
      // Sender:
      //
      
      int const
         numbers_to_send[3] = { 42, 42, 42 };
      
      MPI_Send
         (
         &numbers_to_send,
         3,
         MPI_INT,
         1, 0,
         MPI_COMM_WORLD
         );
      
      ::std::cout &lt;&lt; "Process 0 sent 3 numbers to process "
                     "1"
                  &lt;&lt; ::std::endl
                     ;
   }
   else if
      (
         proc_rank == 1
      )
   {
      //
      // Receiver:
      //
      
      int
         numbers_received[10];
      
      //
      // Pass this to MPI_Recv to be populated with
      // information about the sender, and information
      // about the length of the message:
      //
      
      MPI_Status
         status;
      
      MPI_Recv
         (
         &numbers_received,
         sizeof(numbers_received) / sizeof(int),
         MPI_INT,
         0, 0,
         MPI_COMM_WORLD,
         &status
         )
         ;
      
      int
         length_of_message(0);
      
      //
      // Get the length of the message from the status
      // object:
      //
      
      MPI_Get_count(&status, MPI_INT, &length_of_message);
      
      ::std::cout &lt;&lt; "Process 1 received "
                  &lt;&lt; length_of_message
                  &lt;&lt; " numbers from process 0"
                  &lt;&lt; ::std::endl
                     ;
   }
   
   MPI_Finalize();
   
   return 0;
}
</code></pre>
         
         <h2 id="attaching_a_debugger">
            Attaching a Debugger
         </h2>
         
         <p>
         
         Create a small MPI application (as follows) that
         sleeps for 1 minute and then prints the rank of the
         MPI process:
         
         </p>
         
         <pre><code>#include &lt;mpi.h&gt;
#include &lt;chrono&gt;
#include &lt;thread&gt;
#include &lt;iostream&gt;

int main(int argc, char** argv)
{
   MPI_Init(NULL, NULL);
   
   int
      proc_rank;
   
   MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);
   
   ::std::this_thread::sleep_for
      (
      ::std::chrono::minutes(1)
      )
      ;
   
   ::std::cout &lt;&lt; "Rank: "
               &lt;&lt; proc_rank
               &lt;&lt; ::std::endl
                  ;
   
   MPI_Finalize();
   
   return 0;
}
</code></pre>
         
         <p>
         
         To start the application with a debugger
         (<program_name>gdb</program_name>) <i>attached to
         each process</i>, launch
         <program_name>mpiexec</program_name> as follows:
         
         </p>
         
         <div class="command_line">mpiexec -f ./hosts konsole -e gdb ./application</div>
         
         <p>
         
         To attach a debugger <i>after</i> starting the
         application, launch
         <program_name>mpiexec</program_name> as follows:
         
         </p>
         
         <div class="command_line">mpiexec -f ./hosts ./application</div>
         
         <p>
         Then use <program_name>ps</program_name> to get the
         PIDs of the MPI processes:
         </p>
         
         <div class="command_line">ps fux | grep "mpiexec" -A 3</div>
         
         <p>
         Note down the PID (the number in the second
         column) of the process to which the debugger will
         be attached.
         </p>
         
         <p>
         In this example the PID was 8934. Start
         <program_name>gdb</program_name> and attach it to
         the running process:
         </p>
         
         <div class="command_line">gdb ./application

GNU gdb (Ubuntu 12.1-0ubuntu1~22.04) 12.1
Copyright (C) 2022 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
...
Reading symbols from ./application...
(gdb) attach 8934
(gdb) break application.cpp:21
(gdb) cont
         </div>
         
         <p>
         You may need to run
         <program_name>gdb</program_name> with 
         <program_name>sudo</program_name> to attach the
         debugger.
         </p>
         
         <h2 id="probing_message_sizes">
            Probing Message Sizes
         </h2>
         
         <p>
         In <a href="#sending_mpi_messages">example 2</a>
         we used <function_name>MPI_Send</function_name> and
         <function_name>MPI_Receive</function_name> to send
         data directly from one process to another. Both of
         these functions block until the message has been
         transferred.
         </p>
         
         <p>
         How can the receiver determine the length of
         the incoming message before receiving it?
         </p>
         
         <p>
         Use
         <a href="https://www.mpich.org/static/docs/v3.1/www3/MPI_Probe.html">MPI_Probe</a> to get the
         <function_name>MPI_Status</function_name> of the
         incoming message, then allocate an array of the
         required size, then call
         <function_name>MPI_Receive</function_name>, as
         follows:
         </p>
         
         <pre><code>#include &lt;mpi.h&gt;
#include &lt;iostream&gt;
#include &lt;memory&gt;

int main(int argc, char** argv)
{
   MPI_Init(NULL, NULL);
   
   int
      world_size;
   
   MPI_Comm_size(MPI_COMM_WORLD, &world_size);
   
   int
      proc_rank;
   
   MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);
   
   if (
         proc_rank == 0
      )
   {
      //
      // Sender:
      //
      
      int const
         numbers_to_send[3] = { 42, 42, 42 };
      
      MPI_Send
         (
         &numbers_to_send,
         3,
         MPI_INT,
         1, 0,
         MPI_COMM_WORLD
         );
      
      ::std::cout &lt;&lt; "Process 0 sent 3 numbers to process "
                     "1"
                  &lt;&lt; ::std::endl
                     ;
   }
   else if
      (
         proc_rank == 1
      )
   {
      //
      // Receiver:
      //
      
      MPI_Status
         status;
      
      //
      // Probe the size of the incoming message:
      //
      
      MPI_Probe
         (
         0, 0,
         MPI_COMM_WORLD,
         &status
         )
         ;
      
      int
         length_of_message(0);
      
      //
      // Get the length of the message from the status
      // object:
      //
      
      MPI_Get_count(&status, MPI_INT, &length_of_message);
      
      ::std::cout &lt;&lt; "Process 1 will receive "
                  &lt;&lt; length_of_message
                  &lt;&lt; " numbers from process 0"
                  &lt;&lt; ::std::endl
                     ;
      
      ::std::shared_ptr
         &lt;
         int[]
         &gt;
         numbers_received
            (
            new int[length_of_message]
            )
            ;
      
      MPI_Recv
         (
         numbers_received.get(),
         length_of_message,
         MPI_INT,
         0, 0,
         MPI_COMM_WORLD,
         &status
         )
         ;
      
      ::std::cout &lt;&lt; "Process 1 received "
                  &lt;&lt; length_of_message
                  &lt;&lt; " numbers from process 0"
                  &lt;&lt; ::std::endl
                     ;
   }
   
   MPI_Finalize();
   
   return 0;
}
</code></pre>
         
         <h2 id="broadcasting_data">
            Broadcasting Data
         </h2>
         
         <p>
         Broadcasting in MPI is the act of sending data from one process to <i>all other</i> processes.
         </p>
         
         <p>
         This can be accomplished using 
         <function_name>MPI_Send</function_name> N-1 times,
         but the MPI standard contains a dedicated function
         for broadcasting called
         <function_name>MPI_Bcast</function_name>.
         </p>
         
         <p>
         <function_name>MPI_Bcast</function_name> behaves
         differently depending on whether the process is
         the broadcaster or the receiver. If the process is
         the broadcaster then the first argument to
         <function_name>MPI_Bcast</function_name> is a
         pointer to the value to send. If the process is a
         receiver, then the first argument is a pointer to a
         buffer in which to store the value received.
         </p>
         
         <pre><code>#include &lt;mpi.h&gt;
#include &lt;iostream&gt;
#include &lt;memory&gt;

int main(int argc, char** argv)
{
   MPI_Init(NULL, NULL);
   
   int
      proc_rank;
   
   MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);
   
   int
      broadcast_value = 0;
   
   if (
         proc_rank == 0
      )
   {
      broadcast_value = 42;
   }
   
   MPI_Bcast
      (
      &broadcast_value,
      1,
      MPI_INT,
      0,
      MPI_COMM_WORLD
      )
      ;
   
   if (
         proc_rank != 0
      )
   {
      ::std::cout &lt;&lt; "Process "
                  &lt;&lt; proc_rank
                  &lt;&lt; " received "
                  &lt;&lt; broadcast_value
                  &lt;&lt; " from process 0"
                  &lt;&lt; ::std::endl
                     ;
   }
   
   MPI_Finalize();
   
   return 0;
}
</code></pre>
         
         <p>
         Possible program output:
         </p>
         
         <div class='program_output'>Process 1 received 42 from process 0
Process 4 received 42 from process 0
Process 5 received 42 from process 0
Process 2 received 42 from process 0
Process 3 received 42 from process 0
Process 6 received 42 from process 0</div>
         
         <p>
         The order you see may differ.
         </p>
         
         <h2 id="scattering_data">Scattering Data</h2>
         
         <p>
         <i>Scattering</i> splits a buffer into parts and
         then sends one part to each process. For example:
         if the process with rank 0 owns a buffer containing
         7 elements, and if there are 7 processes, then
         process 0 can <i>scatter 1 element to each
         process</i>.
         </p>
         
         <p>
         Similarly, if process 0 has a buffer of size 21,
         and if there are 7 processes, then process 0 can
         scatter the first 3 elements to itself, the next 3
         elements to process 1, the next 3 elements to
         process 2, and so on until the end of the buffer
         is reached.
         </p>
         
         <p>
         Scattering is performed using
         <a href="https://www.mpich.org/static/docs/v3.2/www3/MPI_Scatter.html">MPI_Scatter</a>. Note that the second
         argument to
         <function_name>MPI_Scatter</function_name> is the
         number of elements to send to each process, not the
         total number of processes.
         </p>
         
         <p>
         In this example the total number of scattered
         elements is divisible by the number of processes:
         </p>
         
         <pre><code>#include &lt;mpi.h&gt;
#include &lt;iostream&gt;
#include &lt;memory&gt;

int main(int argc, char** argv)
{
   MPI_Init(NULL, NULL);
   
   int
      proc_rank;
   
   MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);
   
   ::std::shared_ptr
      &lt;
      int[]
      &gt;
      scattered_values;
   
   int
      received_value = 0;
   
   if (
         proc_rank == 0
      )
   {
      //
      // Only populate this buffer on process 0:
      //
      
      scattered_values.reset
         (
         new int[] { 42, 43, 44, 45, 46, 47, 48 }
         )
         ;
   }
   
   //
   // Send one integer to each process:
   //
   
   MPI_Scatter
      (
      scattered_values.get(),
      1,
      MPI_INT,
      &received_value,
      1,
      MPI_INT,
      0,
      MPI_COMM_WORLD
      )
      ;
   
   ::std::cout &lt;&lt; "Process "
               &lt;&lt; proc_rank
               &lt;&lt; " received "
               &lt;&lt; received_value
               &lt;&lt; " from process 0"
               &lt;&lt; ::std::endl
                  ;
   
   MPI_Finalize();
   
   return 0;
}
</code></pre>
         
         <p>
         Possible program output:
         </p>
         
         <div class='program_output'>Process 0 received 42 from process 0
Process 1 received 43 from process 0
Process 4 received 46 from process 0
Process 5 received 47 from process 0
Process 2 received 44 from process 0
Process 3 received 45 from process 0
Process 6 received 48 from process 0</div>
         
         <p>
         The order you see may differ.
         </p>
         
         <p id="scattering_data_of_variable_lengths">
         If the total number of elements to scatter is not
         divisible by the number of processes, use
         <a href="https://www.mpich.org/static/docs/v3.1/www3/MPI_Scatterv.html">MPI_Scatterv</a> instead of
         <function_name>MPI_Scatter</function_name>.
         </p>
         
         <p>
         <function_name>MPI_Scatterv</function_name>
         differs from
         <function_name>MPI_Scatter</function_name> in that
         the number of records to send to each process must
         be specified explicitly. This means that
         <function_name>MPI_Scatterv</function_name> can
         scatter <i>arrays of variable sizes</i> among the
         receiving processes.
         </p>
         
         <p>
         In this example process 0 scatters 8 records among
         7 processes. Process 6, the last process, receives
         2 records instead of 1:
         </p>
         
         <pre><code>#include &lt;mpi.h&gt;
#include &lt;iostream&gt;
#include &lt;memory&gt;

int main(int argc, char** argv)
{
   MPI_Init(NULL, NULL);
   
   int
      proc_rank;
   
   MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);
   
   ::std::shared_ptr
      &lt;
      int[]
      &gt;
      scattered_values,
      chunk_sizes,
      displacements,
      received_values;
   
   chunk_sizes.reset
      (
      new int[] { 1, 1, 1, 1, 1, 1, 2 }
      )
      ;
   
   received_values.reset
      (
      new int[ chunk_sizes[proc_rank] ]
      )
      ;
   
   if (
         proc_rank == 0
      )
   {
      //
      // Only populate these buffers on process 0:
      //
      
      scattered_values.reset
         (
         new int[] { 42, 43, 44, 45, 46, 47, 48, 49 }
         )
         ;
      
      displacements.reset
         (
         new int[] { 0, 1, 2, 3, 4, 5, 6 }
         )
         ;
   }
   
   //
   // Send one integer to each process, except the last,
   // which receives 2 integers:
   //
   
   MPI_Scatterv
      (
      scattered_values.get(),
      chunk_sizes.get(),
      displacements.get(),
      MPI_INT,
      received_values.get(),
      chunk_sizes[proc_rank],
      MPI_INT,
      0,
      MPI_COMM_WORLD
      )
      ;
   
   ::std::cout &lt;&lt; "Process "
               &lt;&lt; proc_rank
               &lt;&lt; " received "
               &lt;&lt; chunk_sizes[proc_rank]
               &lt;&lt; " datums from process 0: "
                  ;
   
   for(unsigned i(0u); i&lt; chunk_sizes[proc_rank]; ++i)
   {
      ::std::cout &lt;&lt; received_values[i] &lt;&lt; " ";
   }
   
   ::std::cout &lt;&lt; ::std::endl;
   
   MPI_Finalize();
   
   return 0;
}
</code></pre>
         
         <p>
         Possible program output:
         </p>
         
         <div class='program_output'>Process 0 received 1 datums from process 0: 42 
Process 1 received 1 datums from process 0: 43 
Process 2 received 1 datums from process 0: 44 
Process 3 received 1 datums from process 0: 45 
Process 4 received 1 datums from process 0: 46 
Process 5 received 1 datums from process 0: 47 
Process 6 received 2 datums from process 0: 48 49</div>
         
         <p>
         The order you see may differ.
         </p>
         
         <h2 id="gathering_data">
            Gathering Data
         </h2>
         
         <p>
         <i>Gathering</i> is the opposite of scattering.
         When gathering data, one process receives one or
         more elements from every other MPI process.
         </p>
         
         <p>
         When gathering, only one process receives values.
         All other processes are senders.
         </p>
         
         <p>
         For example: if there are 7 processes each having
         one integer to send to process 0, then
         <a href="https://www.mpich.org/static/docs/v3.3/www3/MPI_Gather.html">MPI_Gather</a>
         can be used to accumulate all of the integers on
         process 0. Similarly, if there are 7 processes each
         having 2 integers to send to process 0, then
         <function_name>MPI_Gather</function_name> can be
         used to accumulate all 14 elements on process 0.
         </p>
         
         <p>
         In this example all processes send the same number of elements to process 0:
         </p>
         
         <pre><code>#include &lt;mpi.h&gt;
#include &lt;iostream&gt;
#include &lt;memory&gt;

int main(int argc, char** argv)
{
   MPI_Init(NULL, NULL);
   
   int
      proc_rank;
   
   MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);
   
   ::std::shared_ptr
      &lt;
      int[]
      &gt;
      gathered_values;
   
   if (
         proc_rank == 0
      )
   {
      gathered_values.reset   
         (
         new int[7]
         )
         ;
   }
   
   int
      value_to_send = 42 + proc_rank;
   
   //
   // Gather one integer from each process:
   //
   
   MPI_Gather
      (
      &value_to_send,
      1,
      MPI_INT,
      gathered_values.get(),
      1,
      MPI_INT,
      0,
      MPI_COMM_WORLD
      )
      ;
   
   if (
         proc_rank == 0
      )
   {
      ::std::cout &lt;&lt; "Process 0 gathered 7 values as "
                     "follows: "
                     ;
      
      for(unsigned i(0u); i&lt; 7; ++i)
      {
         ::std::cout &lt;&lt; gathered_values[i] &lt;&lt; " ";
      }
      
      ::std::cout &lt;&lt; ::std::endl;
   }
   
   MPI_Finalize();
   
   return 0;
}
</code></pre>
         
         <p>
         Program output:
         </p>
         
         <div class='program_output'>Process 0 gathered 7 values as follows: 42 43 44 45 46 47 48</div>
         
         <p id="using_mpi_gatherv"> </p>
         
         <p>
         If process 0 receives <i>different numbers
         of elements</i> from each other process, use
         <a href="https://www.mpich.org/static/docs/v3.3/www3/MPI_Gatherv.html">MPI_Gatherv</a>.
         </p>

         <p>
         <function_name>MPI_Gatherv</function_name> differes
         from <function_name>MPI_Gather</function_name> in
         that the number of elements to send per process
         must be specified explicitly.
         </p>
         
         <p>
         In this example process N sends N elements to 
         process 0 using
         <function_name>MPI_Gatherv</function_name>:
         </p>
         
         <pre><code>#include &lt;mpi.h&gt;
#include &lt;iostream&gt;
#include &lt;memory&gt;

int main(int argc, char** argv)
{
   MPI_Init(NULL, NULL);
   
   int
      proc_rank;
   
   MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);
   
   //
   // Process 1 sends 1 element to process 0:
   // Process 2 sends 2 elements to process 0:
   // Process 3 sends 3 elements to process 0 (and so on)
   //
   
   ::std::shared_ptr
      &lt;
      int[]
      &gt;
      gathered_values
         (
         new int[21] // 6 + 5 + ... + 1 + 0
         )
         ,
      displacements
         (
         new int[7] { 0, 0, 1, 3, 6, 10, 15 }
         )
         ,
      receive_counts
         (
         new int[7] { 0, 1, 2, 3, 4, 5, 6 }
         )
         ,
      sent_values;
   
   sent_values.reset( new int [proc_rank] );
   
   for(unsigned i(0u); i&lt; proc_rank; ++i)
   {
      sent_values[i] =
         displacements[proc_rank] + i;
   }
   
   MPI_Gatherv
      (
      sent_values.get(),
      proc_rank,
      MPI_INT,
      gathered_values.get(),
      receive_counts.get(),
      displacements.get(),
      MPI_INT,
      0,
      MPI_COMM_WORLD
      )
      ;
   
   if (
         proc_rank == 0
      )
   {
      ::std::cout &lt;&lt; "Process "
                  &lt;&lt; proc_rank
                  &lt;&lt; " received 21 datums: "
                     ;
      
      for(unsigned i(0u); i&lt; 21; ++i)
      {
         ::std::cout &lt;&lt; gathered_values[i] &lt;&lt; " ";
      }
      
      ::std::cout &lt;&lt; ::std::endl;
   }
   
   MPI_Finalize();
   
   return 0;
}
</code></pre>
         
         <p>
         Program output:
         </p>
         
         <div class='program_output'>Process 0 received 21 datums: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20</div>
         
         </p>
         
         <h2 id="using_mpi_allgather">
            Using MPI_Allgather
         </h2>
         
         <p>
         <a href="https://www.mpich.org/static/docs/v3.2/www3/MPI_Allgather.html">MPI_Allgather</a> behaves like
         <function_name>MPI_Gather</function_name> followed
         by a broadcast
         (<function_name>MPI_BCast</function_name>). All
         values are gathered to all nodes.
         </p>
         
         <p>
         For example: if there are 3 processes and if node 0
         has value 1, node 1 has value 2 and node 2 has
         value 3, then
         <function_name>MPI_Allgather</function_name> will
         collect all of the values to all of the nodes: node
         0 will contain values 1 2 and 3, and so will nodes
         1 and 2.
         </p>
         
         <pre><code>#include &lt;mpi.h&gt;
#include &lt;iostream&gt;
#include &lt;memory&gt;
#include &lt;sstream&gt;

int main(int argc, char** argv)
{
   MPI_Init(NULL, NULL);
   
   int
      world_size;
   
   MPI_Comm_size(MPI_COMM_WORLD, &world_size);
   
   int
      proc_rank;
   
   MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);
   
   unsigned const
      number_of_values_per_proc = 2u;
   
   ::std::shared_ptr
      &lt;
      int[]
      &gt;
      values_to_send
         (
         new int[number_of_values_per_proc]
         )
         ;
   
   for(unsigned i(0u); i&lt; number_of_values_per_proc; ++i)
   {
      values_to_send[i] =
         proc_rank * number_of_values_per_proc + i;
   }
   
   ::std::cout &lt;&lt; "Process "
               &lt;&lt; proc_rank
               &lt;&lt; " created the following values: "
                  ;
   
   for(unsigned i(0u); i&lt; number_of_values_per_proc; ++i)
   {
      ::std::cout &lt;&lt; values_to_send[i] &lt;&lt; " ";
   }
   
   ::std::cout &lt;&lt; ::std::endl;
   
   ::std::shared_ptr
      &lt;
      int[]
      &gt;
      received_values
         (
         new int[world_size * number_of_values_per_proc]
         )
         ;
   
   MPI_Allgather
      (
      values_to_send.get(),
      2u,
      MPI_INT,
      received_values.get(),
      2u,
      MPI_INT,
      MPI_COMM_WORLD
      )
      ;
   
   ::std::stringstream
      ss;
   
   ss &lt;&lt; "Process "
      &lt;&lt; proc_rank
      &lt;&lt; " received the following values: "
         ;
   
   for(
      unsigned i(0u);
      i&lt; world_size * number_of_values_per_proc;
      ++i
      )
   {
      ss &lt;&lt; received_values[i] &lt;&lt; " ";
   }
   
   ss &lt;&lt; ::std::endl;
   
   ::std::cout &lt;&lt; ss.str();
   
   MPI_Finalize();
   
   return 0;
}
</code></pre>
         
         <p>Possible program output:</p>
         
         <div class="program_output">Process 0 created the following values: 0 1 
Process 1 created the following values: 2 3 
Process 2 created the following values: 4 5 
Process 0 received the following values: 0 1 2 3 4 5 
Process 1 received the following values: 0 1 2 3 4 5 
Process 2 received the following values: 0 1 2 3 4 5</div>
         
         <p>The order you see may differ.</p>
         
         <h2 id="parallel_reduction">Parallel Reduction</h2>
         
         <p>
         <i>Reducing</i> an array of numbers is to
         <i>map the array to a single number using a
         function</i>.
         </p>
         
         <p>
         For example, the sum function:
         </p>
         <pre><code> sum(array) =  array[0] +  array[1] + … + array[N-1]</code></pre>
         <p>
         maps an array to a number containing the sum of the
         array. Other examples of map functions include
         multiplication and bitwise operations like & and
         |.
         </p>
         
         <p>
         MPI contains many functions that perform reduction.
         In its basic form, there is one element on each
         process and a reduction function is applied to all
         of them. The root process receives the reduced
         value.
         </p>
         
         <p>
         If there is more than one element per process then
         reduction is applied to all of the elements in
         slot 0, and (separately) to all of the elements in
         slot 1, and so on, up until the end of the arrays.
         The root process receives an array containing the
         reduced elements in slot 0, the reduced elements in
         slot 1, and so on.
         </p>
         
         <p>
         MPI reduction is performed using
         <a href="https://www.mpich.org/static/docs/v3.3/www3/MPI_Reduce.html">MPI_Reduce</a>.
         </p>
         
         <p>
         The meaning of the arguments to
         <function_name>MPI_Reduce</function_name> are as
         follows:
         </p>
         
         <pre><code>int MPI_Reduce
   (
   const void * sendbuf,
   void *recvbuf,
   int count,
   MPI_Datatype datatype,
   MPI_Op op,
   int root,
   MPI_Comm comm
   )
</code></pre>
         
         <ul>
         <li> <i>sendbuf</i> is a buffer containing the
              value(s) to send to the root process
         <li> <i>count</i> is the number of elements in
              <i>sendbuf</i>
         <li> <i>datatype</i> is the type of data in
              <i>sendbuf</i> (eg. MPI_INT)
         <li> <i>op</i> is the reduction operation to
              perform
         <li> <i>root</i> is the rank of the root process,
              which will receive the reduced value(s)
         <li> <i>comm</i> is the communicator (eg.
              MPI_COMM_WORLD)
         </ul>
         
         <p>
         There are many possible values of <i>op</i>,
         including: <function_name>MPI_MAX</function_name>
         (the value of the maximum element),
         <function_name>MPI_MIN</function_name> (the value
         of the minimum element),
         <function_name>MPI_SUM</function_name> (the sum of
         the values),
         <function_name>MPI_PROD</function_name> (the
         multiplicative product of values),
         <function_name>MPI_LAND</function_name> (the
         logical AND/&& of values),
         <function_name>MPI_BAND</function_name>
         (the bitwise AND/& of values),
         <function_name>MPI_LOR</function_name> (the logical
         OR/|| of values),
         <function_name>MPI_BOR</function_name> (the binary
         OR/| of values),
         <function_name>MPI_LXOR</function_name> (the
         logical XOR of values),
         <function_name>MPI_BXOR</function_name> (the binary
         XOR of values),
         <function_name>MPI_MINLOC</function_name>
         (the minimum value and the rank of the process that
         provided it), and
         <function_name>MPI_MAXLOC</function_name> (the
         maximum values and the rank of the process that
         provided it).
         </p>
         
         <p>
         This example uses
         <function_name>MPI_MAXLOC</function_name> to find
         the maximum value of data on 7 processes, plus
         the rank of the process that contained the maximum
         value:
         </p>
         
         <pre><code>#include &lt;mpi.h&gt;
#include &lt;iostream&gt;
#include &lt;memory&gt;
#include &lt;algorithm&gt;

int main(int argc, char** argv)
{
   MPI_Init(NULL, NULL);
   
   int
      proc_rank;
   
   MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);
   
   //
   // Create 10 integers per process, and populate them
   // with values:
   //
   
   unsigned const
      number_of_values_per_proc = 10;
   
   ::std::shared_ptr
      &lt;
      int[]
      &gt;
      values
         (
         new int[number_of_values_per_proc]
         )
         ;
   
   for(unsigned i(0u); i&lt; number_of_values_per_proc; ++i)
   {
      values[i] =
         proc_rank * number_of_values_per_proc + i;
   }
   
   //
   // Compute the maximum value among integers belonging
   // to this process:
   //
   
   int
      maximum_value[2] = { 0, proc_rank };
   
   maximum_value[0] =
      *::std::max_element
         (
         values.get(),
         values.get() + number_of_values_per_proc
         )
         ;
   
   int
      reduced_data[2];
   
   //
   // Reduce the maximum values:
   //
   
   MPI_Reduce
      (
      &maximum_value,
      reduced_data,
      1,
      MPI_2INT,
      MPI_MAXLOC,
      0,
      MPI_COMM_WORLD
      )
      ;
   
   if (
         proc_rank == 0
      )
   {
      ::std::cout &lt;&lt; "The maximum value, "
                  &lt;&lt; reduced_data[0]
                  &lt;&lt; ", was found on process "
                  &lt;&lt; reduced_data[1]
                  &lt;&lt; ::std::endl
                     ;
   }
   
   MPI_Finalize();
   
   return 0;
}
</code></pre>
         
         <h2 id="using_mpi_reduce_scatter">
            Using MPI_Reduce_scatter
         </h2>
         
         <p>
         <a href="https://www.mpich.org/static/docs/v3.3/www3/MPI_Reduce_scatter.html">MPI_Reduce_scatter</a> behaves like
         <function_name>MPI_Reduce</function_name> followed
         by a scatter operation
         (<function_name>MPI_Scatter</function_name>).
         </p>
         
         <p>
         If there is more than one reduced value (because
         there was more than one value per process) then the
         reduced values can be scattered among processes.
         The scattering can be inhomogeneous: some processes
         can receive more of the reduced values than others.
         </p>
         
         <p>
         For example, if we have the following arrangement:
         </p>
         
         <ul>
         <li> Process 0 owns the values 0 1 2 3,
         <li> Process 1 owns the values 4 5 6 7,
         <li> Process 2 owns the values 8 9 10 11,
         </ul>
         
         <p>
         then we can reduce to 4 values (being
         8=<function_name>max(0,4,8)</function_name>,
         9=<function_name>max(1,5,9)</function_name>,
         10=<function_name>max(2,6,10)</function_name> and
         11=<function_name>max(3,7,11)</function_name>) and
         then scatter the results back to the 3 processes as
         follows:
         </p>
         
         <ul>
         <li> Process 0 receives the value 8,
         <li> Process 1 receives the values 9 and 10,
         <li> Process 2 receives the value 11.
         </ul>
         
         <pre><code>#include &lt;mpi.h&gt;
#include &lt;iostream&gt;
#include &lt;memory&gt;
#include &lt;sstream&gt;

int main(int argc, char** argv)
{
   MPI_Init(NULL, NULL);
   
   int
      world_size;
   
   MPI_Comm_size(MPI_COMM_WORLD, &world_size);
   
   int
      proc_rank;
   
   MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);
   
   unsigned const
      number_of_values_per_proc = 4;
   
   ::std::shared_ptr
      &lt;
      int[]
      &gt;
      values_to_reduce
         (
         new int[number_of_values_per_proc]
         )
         ;
   
   for(unsigned i(0u); i&lt; number_of_values_per_proc; ++i)
   {
      values_to_reduce[i] =
         proc_rank * number_of_values_per_proc + i;
   }
   
   ::std::cout &lt;&lt; "Process "
               &lt;&lt; proc_rank
               &lt;&lt; " created the following values: "
                  ;
   
   for(unsigned i(0u); i&lt; number_of_values_per_proc; ++i)
   {
      ::std::cout &lt;&lt; values_to_reduce[i] &lt;&lt; " ";
   }
   
   ::std::cout &lt;&lt; ::std::endl;
   
   ::std::shared_ptr
      &lt;
      int[]
      &gt;
      counts
         (
         new int[] { 1, 2, 1 }
         )
         ;
   
   ::std::shared_ptr
      &lt;
      int[]
      &gt;
      received_values
         (
         new int[ counts[proc_rank] ]
         )
         ;
   
   MPI_Reduce_scatter
      (
      values_to_reduce.get(),
      received_values.get(),
      counts.get(),
      MPI_INT,
      MPI_MAX,
      MPI_COMM_WORLD
      )
      ;
   
   ::std::stringstream
      ss;
   
   ss &lt;&lt; "Process "
      &lt;&lt; proc_rank
      &lt;&lt; " received the following values: "
         ;
   
   for(
      unsigned i(0u);
      i&lt; counts[proc_rank];
      ++i
      )
   {
      ss &lt;&lt; received_values[i] &lt;&lt; " ";
   }
   
   ss &lt;&lt; ::std::endl;
   
   ::std::cout &lt;&lt; ss.str();
   
   MPI_Finalize();
   
   return 0;
}
</code></pre>
         
         <p>Possible program output:</p>
         
         <div class="program_output">Process 0 created the following values: 0 1 2 3 
Process 1 created the following values: 4 5 6 7 
Process 2 created the following values: 8 9 10 11 
Process 0 received the following values: 8 
Process 1 received the following values: 9 10 
Process 2 received the following values: 11</div>
         
         <p>The order you see may differ.</p>
         
         <h2 id="all_to_all">All to All</h2>
         
         <p>
         <a href="https://www.mpich.org/static/docs/v3.1/www3/MPI_Alltoall.html">MPI_Alltoall</a>
         is a combination of
         <function_name>MPI_Gather</function_name> and
         <function_name>MPI_Scatter</function_name>. 
         In this case, every node scatters data <i>to every
         other node</i>. 
         </p>
         
         <p>
         For example: if there are 3 processes and node 0
         contains the values 0, 1, 2, node 1 contains the
         values 3, 4, 5 and node 2 contains the values
         6, 7, 8 then 
         <function_name>MPI_Alltoall</function_name> will:
         </p>
         
         <ul>
            <li>scatter the value 0 to node 0, 
            <li>scatter the value 1 to node 1,
            <li>scatter the value 2 to node 2,
            <li>scatter the value 3 to node 0,
            <li>scatter the value 4 to node 1,
            <li>(and so on)
         </ul>

         <p>
         This essentially performs a matrix transpose
         operation: after
         <function_name>MPI_Alltoall</function_name>, node 0
         contains the values 0, 3 and 6, node 1 contains the
         values 1, 4 and 7, and node 1 contains the values
         2, 5 and 8.
         </p>
         
         <pre><code>#include &lt;mpi.h&gt;
#include &lt;iostream&gt;
#include &lt;memory&gt;

int main(int argc, char** argv)
{
   MPI_Init(NULL, NULL);
   
   int
      world_size;
   
   MPI_Comm_size(MPI_COMM_WORLD, &world_size);
   
   int
      proc_rank;
   
   MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);
   
   unsigned const
      number_of_values_per_proc = world_size * 2u;
   
   ::std::shared_ptr
      &lt;
      int[]
      &gt;
      values_to_send
         (
         new int[number_of_values_per_proc]
         )
         ;
   
   for(unsigned i(0u); i&lt; number_of_values_per_proc; ++i)
   {
      values_to_send[i] =
         proc_rank * number_of_values_per_proc + i;
   }
   
   ::std::cout &lt;&lt; "Process "
               &lt;&lt; proc_rank
               &lt;&lt; " created the following values: "
                  ;
   
   for(unsigned i(0u); i&lt; number_of_values_per_proc; ++i)
   {
      ::std::cout &lt;&lt; values_to_send[i] &lt;&lt; " ";
   }
   
   ::std::cout &lt;&lt; ::std::endl;
   
   ::std::shared_ptr
      &lt;
      int[]
      &gt;
      received_values
         (
         new int[number_of_values_per_proc]
         )
         ;
   
   MPI_Alltoall
      (
      values_to_send.get(),
      2u,
      MPI_INT,
      received_values.get(),
      2u,
      MPI_INT,
      MPI_COMM_WORLD
      )
      ;
   
   ::std::cout &lt;&lt; "Process "
               &lt;&lt; proc_rank
               &lt;&lt; " received the following values: "
                  ;
   
   for(unsigned i(0u); i&lt; number_of_values_per_proc; ++i)
   {
      ::std::cout &lt;&lt; received_values[i] &lt;&lt; " ";
   }
   
   ::std::cout &lt;&lt; ::std::endl;
   
   MPI_Finalize();
   
   return 0;
}
</code></pre>
         
         <p>
         Possible program output:
         </p>
         
         <div class='program_output'>Process 0 created the following values: 0 1 2 3 4 5 
Process 1 created the following values: 6 7 8 9 10 11 
Process 2 created the following values: 12 13 14 15 16 17 
Process 0 received the following values: 0 1 6 7 12 13
Process 1 received the following values: 2 3 8 9 14 15
Process 2 received the following values: 4 5 10 11 16 17</div>
         
         <p>
         The order you see may vary.
         </p>
         
         <h2 id="nonblocking_send_receive">
            Nonblocking Send/Receive, and Barriers
         </h2>
         
         <p>
         <a href="https://www.mpich.org/static/docs/v3.1/www3/MPI_Send.html">MPI_Send</a> is a <i>blocking</i>
         send operation. It may block until the message it
         passes is received by the receiving
         process. <a href="https://www.mpich.org/static/docs/v3.1/www3/MPI_Recv.html">MPI_Recv</a>, likewise, is a blocking
         receive operation.
         </p>
         
         <p>
         There are <i>non-blocking</i> variants of both of
         these functions named <a href="https://www.mpich.org/static/docs/v3.1/www3/MPI_Isend.html">MPI_Isend</a> and <a href="https://www.mpich.org/static/docs/v3.1/www3/MPI_Irecv.html">MPI_Irecv</a>. These
         functions return control to the caller immediately.
         In general, the underlying MPI operation will not
         have completed before these functions return
         control to their callers.
         </p>
         
         <p>
         The last argument to <function_name>
         MPI_Isend</function_name> (and to
         <function_name>
         MPI_Irecv</function_name>) is a pointer to an
         object of type
         <function_name>MPI_Request</function_name>. This
         <i>request object</i> can be used to query the
         status (completed or still pending) of the message
         passing operation. Use
         <function_name>MPI_Wait</function_name> to query
         the status.
         </p>
         
         <p>
         Using non-blocking communication functions can
         cause process synchronization to diverge. Use
         <function_name>MPI_Barrier</function_name> to
         synchronize processes.
         <function_name>MPI_Barrier</function_name> will
         block until all of the processes have reached the
         barrier function, then all of the blocked processes 
         will continue.
         </p>
         
         <pre><code>#include &lt;mpi.h&gt;
#include &lt;iostream&gt;

int main(int argc, char** argv)
{
   MPI_Init(NULL, NULL);
   
   int
      proc_rank;
   
   MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);
   
   MPI_Request
      request;
   
   if (
         proc_rank == 0
      )
   {
      int const
         number_to_send = 42;
      
      //
      // Non-blocking send:
      //
      
      MPI_Isend
         (
         &number_to_send,
         1,
         MPI_INT,
         1, 0,
         MPI_COMM_WORLD,
         &request
         );
      
      ::std::cout &lt;&lt; "Process 0 sent "
                  &lt;&lt; number_to_send
                  &lt;&lt; " to process 1"
                  &lt;&lt; ::std::endl
                     ;
   }
   else if
      (
         proc_rank == 1
      )
   {
      int
         number_received = 0;
      
      //
      // Non-blocking receive:
      //
      
      MPI_Irecv
         (
         &number_received,
         1,
         MPI_INT,
         0, 0,
         MPI_COMM_WORLD,
         &request
         )
         ;
      
      ::std::cout &lt;&lt; "Process 1 received "
                  &lt;&lt; number_received
                  &lt;&lt; " from process 0"
                  &lt;&lt; ::std::endl
                     ;
   }
   
   //
   // Wait for the request to complete:
   //
   
   MPI_Wait(&request, MPI_STATUS_IGNORE);
   
   //
   // Synchronize the processes:
   //
   
   MPI_Barrier(MPI_COMM_WORLD);
   
   MPI_Finalize();
   
   return 0;
}
</code></pre>
         
         <h2>Licenses</h2>
         
         <p>
         
         All of the code in this repository (including in
         documentation and in README.md files and in HTML
         files) is licensed under the GNU General Public
         License, version 3. See
         <a href="https://www.gnu.org/licenses/">
            https://www.gnu.org/licenses/gpl-3.0.html
         </a>.
         
         </p>
         
         <p>
         
         All of the text in this repository (including in
         README.md files and in webpages) is licensed under
         the GNU Free Documentation License. See
         <a href="https://www.gnu.org/licenses/fdl-1.3.en.html">
            https://www.gnu.org/licenses/fdl-1.3.en.html
         </a>.
         
         </p>
         
         <p>
         
         The C++ code snippets on this website can be found
         at
         <a href="https://github.com/j040222/mpi-tutorial/tree/main/code">
            https://github.com/j040222/mpi-tutorial/tree/main/code
         </a>.
         
         </p>
         
      </div>
   </body>
   
   <script>
      hljs.highlightAll();
   </script>
   
</html> 
